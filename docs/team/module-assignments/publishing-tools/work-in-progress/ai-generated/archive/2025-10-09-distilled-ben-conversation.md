# Publishing Tools Module: Distilled Requirements

**Source:** `/Users/grig/work/peermesh/repo/knowledge-graph-lab-alpha/docs/team/module-assignments/publishing-tools/work-in-progress/raw/2025-10-01-spark-chat-ben+grig.md`

**Distilled:** 2025-10-09

**Status:** Awaiting Human Review

**Module:** publishing-tools

---

## 1. MVP Features Agreement

### Explicitly Agreed for MVP

**Email Publishing:** Primary channel confirmed for MVP
> "The emails sounds like a good one for sure"

**Template System:** Basic templating with placeholders/shortcodes
> "We talked about a templating system where we would have, we only need one template, but maybe two"

**Unsubscribe Flow:** Required for legal compliance
> "To be compliant with law, we should have the unsubscribe button, the link there, which means anyone who's had enough can click a button to unsubscribe"

**Dispatch Logging:** Track what was sent and when
> "That dispatch log will also have the details of the whole pipeline from the moment it was requested, the moment it entered the system to become something that needed to be done, the moment it was processed, what logic was used for processing, what templates were used"

**Event-Triggered Publishing:** Server calls `doPublish()` at intervals
> "The server will say something like, publish all current, like do publish will be the call"

### Discussed but Deferred to Post-MVP

**Push Notifications to Phone:**
> "Maybe we can get to push notifications or text messages. I think that'd be super cool"
- Status: "Nice to have"

**Text Message Publishing:**
> "Maybe we can get to push notifications or text messages"
- Status: Mentioned as possibility

**Blog Platform Publishing:** Preferred over Instagram
> "What might make more sense than Instagram posts for this would be a publishing tool that publishes to blog platforms. Substack or Patreon or something like that through their API"
- Status: Discussed as alternative, not confirmed for MVP

**Social Sharing/Subscription Features:**
> "Maybe we could even show who else is subscribed... That could be like a really good way to add people to that to the order for the publishing tool"
- Status: Future enhancement

**Dashboard for Email Management:**
> "There's probably ways to, like with a front end, like admin dashboard type thing, look at the emails that have come in, the emails that are sent, you know, track the users separate from the content"
- Status: Post-MVP research needed

### Scope Notes

- Initial focus: Email as single channel
- Blog publishing treated as "same logic, different API endpoint"
- Multiple channel support architecture planned but not MVP requirement

---

## 2. Technical Implementation Decisions

### Decision: Docker Containerization

**What was decided:** Publishing module will be a Docker container

**Rationale:** Modular architecture for PeerMesh system, allows independent deployment and connection to other modules

**Implementation notes:** Module must be self-contained, communicate with other modules via APIs

**Quote:**
> "The module itself will be a Docker container with just your part of the work in. And when we're done, we're going to be connecting that to the other Docker containers"

### Decision: Event-Triggered Scheduling

**What was decided:** Server triggers `doPublish()` method at intervals, not cron-based

**Rationale:** Centralized scheduling managed by main server, publishing module responds to events

**Implementation notes:** Publishing logic must check intervals and decide whether to publish on each trigger

**Quote:**
> "We'll have some sort of main method that's called by the server. And the server will say something like, publish all current, like do publish will be the call"

**Scheduling Logic:**
> "So like that do publish method knows the interval that it's going to be called at. So it could be like, well, I'm supposed to send this guy something every week. It's been six days. The next time I'll be called is tomorrow. So I'll wait till tomorrow"

### Decision: Template-Based Email Rendering

**What was decided:** Use template system with placeholders/shortcodes for customization

**Rationale:** Avoid hard-coding text, support multiple templates, allow personalization

**Implementation notes:**

- At least one template required, ideally two (plain text + styled)
- Shortcodes for user personalization (first name, last name, etc.)
- Templates include header, body sections, footer with unsubscribe

**Quote:**
> "So what the template needs to have is pretty basic. It'll probably have maybe five or six placeholders in it in total, like how you address the user using shortcodes, which is a way of saying insert the user's first name or insert their last name or something like that"

**Template Requirements:**
> "We'll have probably some restrictions on how much of the text gets shown in the email... And then we probably need a secondary template which will be what each report would be"

### Decision: Content Length Management

**What was decided:** Limit email content length, provide "read more" link to full content

**Rationale:** Keep emails scannable, avoid overwhelming users with long articles

**Implementation notes:**

- ~200 words maximum in email excerpt
- "Click here for more" button links to full content on server
- Full content rendered dynamically when accessed

**Quote:**
> "So if you've got an article that's hundreds of pages long, you just want the first paragraph or the first, I don't know what's reasonable, 200 words or something. And then so if something's larger than 200 words, we'll have a click here for more button"

### Decision: Dynamic Rendering for Web View

**What was decided:** Render full content on-demand when user clicks "read more", don't pre-render static pages

**Rationale:** Save storage space, allow template changes to apply retroactively, enable customization

**Implementation notes:** Same rendering logic handles email and web view with flags/variables to distinguish output format

**Quote:**
> "Instead of it just being a rendered HTML page, static page, we may want to do something where that page is actually rendered when they open it. So like it does kind of the same logic that we're doing to assemble the email, but it does it on the fly when they ask for the page"

**Benefit:**
> "Because there's no point in saving these pages fully rendered as well as the, you know, chunks. We're already saving the like parts. We don't also want to like have to save a copy that's the full version of it, like, because it doubles the amount of space required"

### Decision: Amazon SES for Email Delivery

**What was decided:** Use Amazon Simple Email Service (SES) for sending emails

**Rationale:** Mentioned as external service for email delivery

**Implementation notes:** Publishing module calls SES API to send composed emails

**Quote:**
> "And when it was sent to the location to be emailed out, in this case, SES by Amazon, the simple email service"

**Email markup:** Format/language for emails mentioned but not specified
> "I think there's a markup language for emails. Did you, I don't know if you ran into that or not"

### Decision: Temporary File Assembly

**What was decided:** Assemble email content in temporary file location before sending

**Rationale:** Avoid keeping everything in RAM, allow file-based processing

**Implementation notes:**

- Copy markdown files to temporary folder
- Apply templates
- Render final email
- Save temporarily for both email send and web view
- Cleanup after dispatch

**Quote:**
> "We'll need some sort of like method for like bringing these, the bits of data we're about to send to a temporary file location, a folder where we assemble the stuff that we're about to send out"

### Decision: State Tracking in User Request Data

**What was decided:** Track publishing state (in-progress, published) in user request data

**Rationale:** Single source of truth for audit trail, status tracking, error logging

**Implementation notes:**

- User request data structure evolves over time
- Contains: user ID, interests/tags, publishing preferences, intervals, channels
- Adds: processing status, delivery logs, error messages, timestamps
- Enables audit: "did they get it? what did they get? was it received?"

**Quote:**
> "The user's request data is going to evolve over time to the point where it's got all of the information. We'll be able to audit it at any moment and be like, okay, did they get it? What did they get? Was it received?"

**Flag States:**
> "And it will then run through and actually do the main body of the work. At the end of the main body of the work, it will set the flag in that data to published"

---

## 3. Integration Contracts

### AI Module Integration

**Purpose:** Publishing module requests article content generated for user's interests

**Direction:** Publishing â†’ AI (request/response)

**What Publishing Needs from AI:**

- Markdown files with article content
- Tagged/categorized by topics
- Filtered by user's interest tags
- Exclude previously sent articles (deduplication)

**Data Format:** TBD - Specific schema not discussed in detail

**What we know:**
> "The AI module has had a chance by this point to also review these requests. So its job was to gather the data. It's like constantly gathering data on the internet and building up this wealth of information. And then when a user asks for something, the AI gets a notification and one of its processes will actually write an article for us"

**Article Format:**
> "And that article will be like a well-formatted article. It may or may not have images associated with it at that point. Probably not. It'll probably just be like a markdown file"

**Storage Location:**
> "Then we'll have another database with all the formalized things we're going to send"

**Deduplication Requirement:**
> "We're going to make a request to the other database for any available stuff that matches those tags that isn't in the list of things we've already sent them. So we don't want to send them last week's stuff"

**Open Question:** Exact API contract, JSON schema, field names not specified

### Backend Module Integration

**Purpose:** Publishing module retrieves user request data and preferences

**Direction:** Publishing â†’ Backend (request/response)

**What Publishing Needs from Backend:**

- User's request data (interests, tags, topics)
- Publishing preferences (channels, intervals, subscription status)
- User identification

**Data Location:**
> "The front end has an interface for choosing where you're publishing, what you're publishing, and that gets sent to the backend for stored into a database for each user. And then this module, the publishing module, will then ask for the data from the database through the core main module"

**Request Format:** JSON expected but not specified
> "The pipeline starts with the sort of sets of instructions in probably JSON file format, where it's, you know, the details about the user and the request"

**User Preference Data:**
> "In that request one, we'll also have things like, I want an email every week. I want you to post on Instagram every day"

**Open Question:** Exact schema, API endpoint, authentication not discussed

### Frontend Module Integration

**Purpose:** User configuration flows to Publishing module, unsubscribe status updates

**Direction:** Bi-directional (Frontend â†’ Backend â†’ Publishing for config; Publishing â†’ Backend â†’ Frontend for status)

**Unsubscribe Flow Integration:**

- User clicks unsubscribe in email
- Takes to page (Frontend) showing subscription summary
- User confirms cancellation
- Frontend sends API call to Backend marking unsubscribed
- Publishing module excludes unsubscribed users in `doPublish()` logic

**Quote:**
> "And the user has clicked the unsubscribe button. That'll take them to a page that says... this is what you asked for. Remember, are you sure you want this to be canceled? And then if they hit cancel, it'll show something on the screen there saying that it's now not going to send any more messages. The system, the dashboard sends an API call to the backend saying, this has been canceled or unsubscribed"

**Open Question:** Does Publishing module expose API for subscription management, or does Frontend write directly to Backend DB?

---

## 4. User Workflows

### Workflow: Email Publishing Pipeline

**Steps:**

1. Server triggers `doPublish()` at regular interval
2. Publishing module queries Backend for active user requests (not unsubscribed)
3. For each user, check if publishing interval has elapsed
4. If ready to publish:
   - Mark request as "in progress"
   - Request articles from AI module matching user's tags
   - Exclude previously sent article IDs (deduplication)
5. Copy article markdown files to temporary assembly folder
6. Apply template formatting
7. Limit content to ~200 words with "read more" link
8. Compose final email
9. Call SES API to send email
10. Log dispatch details (timestamp, template used, articles included, SES response)
11. Mark request as "published"
12. Handle any errors/bounces and log to dispatch log

**Quote (Overall Flow):**
> "So the pipeline starts with the sort of sets of instructions in probably JSON file format, where it's, you know, the details about the user and the request... And we have to figure out how to turn that into stuff that's happening in our pipeline, in the publishing pipeline"

### Workflow: Unsubscribe Process

**Steps:**

1. User receives email with unsubscribe link (token-based, likely)
2. User clicks unsubscribe link
3. Frontend displays confirmation page:
   - Shows summary of what user subscribed to
   - Asks "Are you sure you want this to be canceled?"
4. If user confirms cancellation:
   - Frontend displays confirmation: "Not going to send any more messages"
   - Frontend calls Backend API to mark subscription status
5. Backend updates user request data (unsubscribed flag)
6. Next `doPublish()` call excludes unsubscribed users
7. No more emails sent to that user

**Quote:**
> "And the user has clicked the unsubscribe button. That'll take them to a page that says that'll give them a summary of like, this is what you asked for. Remember, are you sure you want this to be canceled? And then if they hit cancel, it'll shows something on the screen there saying that it's now not going to send any more messages"

**Backend Integration:**
> "The system, the dashboard sends an API call to the backend saying, this has been canceled or unsubscribed. And at that point, when we're asking for what to do, when that first major call that happens, like let's say daily that says do publishing, it will make sure that it's not getting records that have been unsubscribed"

**Open Question:** How is unsubscribe token generated and validated?

### Workflow: "Read More" Web View

**Steps:**

1. User clicks "Click here for more" button in email
2. Link directs to server-hosted page (unique URL per user/digest)
3. Server dynamically renders full content on-demand:
   - Retrieves markdown article files for that digest
   - Applies same template logic as email (with "web page" flag)
   - Renders complete articles without length limits
4. User views full content on web page
5. Future: Page could include interactive elements (save snippet, explore related topics, adjust interests)

**Quote:**
> "It's also going to represent the page that the user is going to see if they click the show me more button. It's going to take them to this custom page that's for them on the content that they were interested in"

**Dynamic Rendering:**
> "Instead of it just being a rendered HTML page, static page, we may want to do something where that page is actually rendered when they open it"

**Future Enhancement:**
> "And in the future, that page could be have some really cool stuff in it. Like, you know, save this snippet for me for later or like let's dive into these topics further"

---

## 5. Technical Specifications

### Database / Data Persistence

**User Request Data (Evolving Structure):**

- User ID
- Research interests (tags/topics as list/array)
- Publishing preferences:
  - Channels enabled (email, blog, push, text)
  - Interval per channel ("every week", "every day")
- Subscription status (active/unsubscribed)
- Processing state ("pending", "in-progress", "published")
- Sent article history (for deduplication)
- Delivery timestamps
- Error logs

**Quote:**
> "The user's request data is going to evolve over time to the point where it's got all of the information. We'll be able to audit it at any moment and be like, okay, did they get it? What did they get? was it received?"

**Open Question:** Database technology not specified (Postgres mentioned elsewhere in project, but not confirmed for Publishing module)

### Dispatch Log Database

**Purpose:** Track every publishing action, API call, and response

**Log Entry Per API Call:**

- Request timestamp (when triggered)
- Processing details:
  - User request ID processed
  - Logic/templates used
  - Articles included (IDs/references)
- Dispatch details:
  - External service called (SES, blog API, etc.)
  - API endpoint
  - Send timestamp
- Response/Status:
  - Success/failure
  - API response data
  - Error messages
  - Bounce notifications (emails)

**Quote:**
> "That dispatch log will also have the details of the whole pipeline from the moment it was requested, the moment it entered the system to become something that needed to be done, the moment it was processed, what logic was used for processing, what templates were used"

**SES Logging:**
> "And when it was sent to the location to be emailed out, in this case, SES by Amazon, the simple email service"

**Multi-Channel Logging:**
> "Each one of those would be its own entry in the log database"

**Post-Send Updates:**
> "For each one of those, there's still a possibility after the call has happened for some sort of issue on the backend or some sort of response from the system"

### Environment / Configuration

**Template System:**

- At least one template required, ideally two
- Templates stored as files (location TBD)
- Shortcode/placeholder support for personalization
- Two-level templates:
  1. Main email template (header, body container, footer)
  2. Individual report template (article formatting)

**Quote:**
> "We'll have probably some restrictions on how much of the text gets shown in the email... And then we probably need a secondary template which will be what each report would be"

**Configuration Needed:**

- Maximum reports per email (configurable)
- Content length limit (~200 words)
- SES credentials/API keys
- Server endpoints for "read more" links
- Unsubscribe token generation settings

**Quote:**
> "And I think we'll probably set the maximum number of reports in an email, either at the template level or in the dashboard for managing the email system"

**Open Question:** Email markup language/format for styling

### File Management

**Temporary Assembly Folder:**

- Location for staging content before send
- Stores markdown files for current publishing batch
- Assembles template + content
- Persists long enough for both email send and web rendering
- Requires cleanup/garbage collection

**Quote:**
> "We'll need some sort of like method for like bringing these, the bits of data we're about to send to a temporary file location, a folder where we assemble the stuff that we're about to send out"

**Storage Considerations:**
> "We really want to be wary of the amount of space these things take up. I'm not quite sure. You know, we'll have to do some math on how many markdown files of final reports of different stuff we can store on a system before it's not economically viable to keep them around anymore"

**Garbage Collection:**
> "Stuff we've sent out that's been around for two years, um, we should be like doing garbage cleanup for, and like making sure that the stuff isn't always there"

**Open Question:** Retention policy not specified (how long to keep old articles?)

---

## 6. Open Questions / Unclear Items

### Questions Still Unanswered

- [ ] **Question:** What exact email markup language/format is used for styled emails?
  - **Context:** Mentioned as existing but not specified
  - **From conversation:** "I think there's a markup language for emails. Did you, I don't know if you ran into that or not"

- [ ] **Question:** Which blog platforms are MVP vs. post-MVP?
  - **Context:** Substack/Patreon mentioned as examples
  - **From conversation:** "Publishing tool that publishes to blog platforms. Substack or Patreon or something like that through their API"
  - **Impact:** If MVP, need API integration specs

- [ ] **Question:** How are publishing intervals stored and calculated?
  - **Context:** "Every week", "every day" mentioned but not data format
  - **Impact:** Affects scheduling logic and data model

- [ ] **Question:** How is unsubscribe token generated and validated?
  - **Context:** Unsubscribe flow described but token mechanism not specified
  - **Impact:** Security and implementation approach

- [ ] **Question:** What is exact JSON schema for AI module article response?
  - **Context:** Known to be markdown files with tags, but structure undefined
  - **Impact:** Can't build integration without schema

- [ ] **Question:** What is exact JSON schema for Backend user request data?
  - **Context:** Known to contain user ID, tags, preferences, but fields undefined
  - **Impact:** Can't build integration without schema

- [ ] **Question:** How many articles per email digest?
  - **Context:** "Maximum number of reports" mentioned as configurable
  - **Impact:** Affects rendering logic and performance

- [ ] **Question:** What database technology for Publishing module data?
  - **Context:** Not discussed
  - **Impact:** Architecture decision needed

- [ ] **Question:** What happens on email bounce or delivery failure?
  - **Context:** Mentioned that bounces need logging but retry logic not discussed
  - **Impact:** Error handling strategy

- [ ] **Question:** How long to retain old article files and dispatch logs?
  - **Context:** Storage concerns mentioned, 2-year example given
  - **Impact:** Garbage collection implementation

- [ ] **Question:** Is there a manual "send now" trigger for testing?
  - **Context:** Not discussed
  - **Impact:** Would be useful for MVP testing/debugging

### Contradictions Found

None identified - conversation was exploratory but consistent in direction.

---

## 7. Critical Quotes

**Vision for User Customization:**
> "The more we get from the user about their interests the curating of the work, we're understanding the user. We're also understanding the space in general, right?"

**System Behavior Philosophy:**
> "These systems don't know anything. So the only information they get is what they get from the user"

**Audit Trail Importance:**
> "The user's request data is going to evolve over time to the point where it's got all of the information. We'll be able to audit it at any moment and be like, okay, did they get it? What did they get? was it received?"

**Multi-Channel Architecture (Future):**
> "We'll have like little flags in that piece of logic that's handling the how do we render this? So the render method, for lack of a better term, will probably be this sort of central set of code that handles that"

**Social/Sharing Potential:**
> "Maybe we could even show who else is subscribed... That could be like a really good way to add people to that to the order for the publishing tool"

**Web View as Public Hub:**
> "The whole internet and all of the search engines will be hitting it too, which turns the entire system into this very large hub of information, which search engines love"

**Email Publishing as Valuable Problem:**
> "There's a buddy who in 2026, I think, was hired into this secret team... Building the email publishing system for Nest... And now he's a self-made millionaire"

---

## 8. References

### Original Conversation

- **File:** `/Users/grig/work/peermesh/repo/knowledge-graph-lab-alpha/docs/team/module-assignments/publishing-tools/work-in-progress/raw/2025-10-01-spark-chat-ben+grig.md`
- **Date:** 2025-10-01
- **Participants:** Ben, Grig
- **Note:** Conversation ended mid-stream ("Call stopped here - we ran out of time")

### Related Documentation Mentioned

- **Amazon SES (Simple Email Service):** Email delivery service
- **Email markup language:** Mentioned but not identified (research needed)
- **Competitive Analysis:** Suggested to research other email publishing tools

### Follow-up Needed

**Technical Research:**

- Email markup languages/formats (MJML, Foundation for Emails, etc.)
- Email publishing system best practices
- Amazon SES integration specifics
- Template engine options (Jinja2, Handlebars, Mustache)

**Clarification Conversations:**

- AI module integration contract (JSON schemas)
- Backend module integration contract (JSON schemas)
- Exact MVP channel scope (email only, or email + blog?)
- Unsubscribe token mechanism
- Error handling and retry strategy

**Alignment Meetings:**

- Frontend team: unsubscribe flow UI/UX
- Backend team: user request data schema
- AI team: article generation format and deduplication

---

## Distillation Quality Self-Check

- [x] All 8 sections present and addressed
- [x] No derived or invented information
- [x] Direct quotes for critical decisions
- [x] Contradictions flagged, not resolved (none found)
- [x] MVP vs. post-MVP clearly separated
- [x] Integration contracts identified (with schemas marked TBD where not discussed)
- [x] Open questions explicitly listed
- [x] Document length ~650 lines (within 400-800 target)
- [x] All extractions traceable to source conversation
- [x] Ready for human review

---

**Status:** Ready for human synthesis review
