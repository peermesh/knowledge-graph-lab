---
gap: "Do commercial platforms (Perplexity AI, Microsoft GraphRAG, LangChain, LlamaIndex, Neo4j, TigerGraph, AWS Kendra, Google Vertex AI, OpenAI Platform, Anthropic Claude) already provide a complete 8-layer autonomous research-enrichment pipeline, or is this a novel composition that requires custom building?"
impact: "This research directly informs critical build-vs-buy decisions. If a complete solution exists, licensing/forking is preferred over 6-month custom development. If partial solutions exist, composition cost analysis is needed. Blocks all architectural decisions until resolved."
tags: [competitive-analysis, market-research, commercial-platforms, perplexity, graphrag, langchain, llamaindex, neo4j, tigergraph, aws-kendra, google-vertex-ai, openai, anthropic, build-vs-buy]
created: 2025-11-13
updated: 2025-11-13T09:45:00
priority: 1
status: pending
research_type: "market analysis + technical evaluation"
scope: "10+ commercial platforms evaluated across 8-layer pipeline completeness"
deliverable: "Comprehensive platform comparison with build vs. buy financial analysis"
findings_summary: ""
---

## Research Metadata

**Research Track:** 00-COMPETITIVE-LANDSCAPE-RESEARCH
**Topic:** Commercial Platforms & Full System Evaluation

### Research Objectives

1. **Complete Systems?** Is there a commercial product doing all 8 layers end-to-end?
2. **Closest Platforms?** Which platforms are closest to our requirements?
3. **Cost Model?** What's the licensing, per-query, and total cost for each?
4. **Customization Burden?** How many engineer-months to reach our quality/cost targets?
5. **Build vs. Buy?** Financial comparison and recommendation

### Success Criteria

- [ ] Clear ranking of platforms by fit for requirements
- [ ] Specific cost numbers for each platform (licensing + per-query)
- [ ] Honest gap assessment vs. our 8-layer design
- [ ] Build vs. buy financial recommendation
- [ ] Top 3-5 platforms recommended for deeper evaluation

### Platforms to Evaluate (Minimum 10)

1. Perplexity AI - Research automation with sources
2. Microsoft GraphRAG - Graph-based RAG system
3. LangChain + LangGraph - Agent orchestration framework
4. LlamaIndex Graph Agents - Index + agent infrastructure
5. Neo4j - Knowledge graph storage (+ construction tools?)
6. TigerGraph - Enterprise knowledge graph platform
7. AWS Kendra - Enterprise knowledge management
8. Google Vertex AI - Unified Google platform
9. OpenAI Platform (GPT-4, Assistants) - Foundation model + APIs
10. Anthropic Claude + API - Alternative LLM foundation

### Research Methodology

**Search Strategy:**
- Official documentation and whitepapers
- Technical deep dives (blog posts, conference talks)
- Hands-on evaluation (sign up, test, reverse-engineer)
- Community feedback (Reddit, HackerNews, LinkedIn)
- Analyst reports (Gartner, Forrester if available)

**Key Search Terms:**
- "[Platform name] knowledge graph construction"
- "[Platform name] research automation"
- "[Platform name] vs [competitor]"
- "[Platform name] cost analysis"
- "[Platform name] customization examples"

**Evaluation Dimensions:**
- Completeness (% of 8 layers covered)
- Accuracy (NER F1, relation extraction, dedup precision)
- Cost (per-query at 10K/day volume)
- Customization effort (engineer-months to target)
- Maturity (production-ready?)
- Maintenance (actively developed?)
- Lock-in (self-hostable?)

### Expected Output

≥4,000 word comprehensive report including:
- Executive summary (build vs. buy recommendation)
- Detailed platform analysis (≥200 words per platform)
- Comparative matrix (all platforms on key dimensions)
- Cost analysis (licensing + per-query comparison)
- Implementation timeline if chosen
- Top 3-5 platforms for deeper POC evaluation
