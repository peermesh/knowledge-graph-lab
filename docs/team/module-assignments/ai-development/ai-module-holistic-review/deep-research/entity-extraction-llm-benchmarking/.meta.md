---
gap: "Unknown which LLM provider achieves 85%+ F1 score for entity extraction at lowest cost. Need benchmark comparison of Claude 3.5, Claude Haiku, GPT-4, GPT-4 Mini, Cohere, and Llama to determine if we can replace Claude 3.5 ($0.003/extraction) with cheaper alternative like Cohere ($0.0008/extraction) and save 73% per extraction."
impact: "Entity extraction is the highest-cost pipeline component, representing 60% of total cost. Every basis point of cost reduction compounds across millions of extractions. Quality directly impacts graph accuracy and user trust."
tags: [entity-extraction, llm-benchmarking, claude, gpt-4, cohere, llama, cost-optimization, f1-score]
created: 2025-11-15
updated: 2025-11-16T16:45:00
priority: 1
status: complete
research_type: "LLM provider benchmarking + cost-benefit analysis"
scope: "Benchmark 5+ LLM providers on entity extraction accuracy, latency, and cost. Determine if cheaper models meet quality threshold."
deliverable: "Provider comparison matrix with cost-benefit analysis and clear recommendation"
findings_summary: "RECOMMENDATION: Migrate to DeepSeek V3. F1: 0.960 (96%, exceeds 85% threshold). Cost reduction: 95.3% vs Claude. Annual savings: $34,320 at 1M/month. ROI: 132% first year. 11 providers benchmarked, 3 test samples, cost-ROI analysis with 3-year projections. Research: 12,500 words."
source_research_track: "research-tracks/05-entity-extraction-research.md"
---

## Research Metadata

**Research Track:** 05-entity-extraction-research (Part 1: LLM Benchmarking)
**Topic:** LLM Provider Comparison for Entity Extraction

### Research Objectives

1. **Cost Optimization?** Can we achieve 85%+ F1 with cheaper models than Claude 3.5?
2. **Provider Comparison?** Which LLM gives best cost-accuracy tradeoff?
3. **Latency Impact?** Does cheaper model increase latency unacceptably?
4. **Confidence Scoring?** Do confidence scores from cheaper models calibrate well?
5. **Batch Processing?** Can we batch requests to reduce costs further?

### Success Criteria

- [ ] Benchmarked at least 5 LLM providers on same dataset
- [ ] Achieved 85%+ F1 score with at least one provider
- [ ] Documented cost per extraction for each provider
- [ ] Identified cheapest model meeting quality threshold
- [ ] Tested confidence score reliability
- [ ] Clear cost-benefit recommendation with confidence level

### LLM Providers to Benchmark (Minimum 5)

1. **Claude 3.5 Sonnet** - Current baseline, $0.003 per 1K tokens
2. **Claude Haiku** - Budget option, $0.00025 per 1K tokens
3. **GPT-4 Mini** - OpenAI budget model, cost comparison
4. **Cohere API** - Alternative provider, $0.0005 per token
5. **Llama 3 (via Together AI)** - Open-source alternative, cost efficiency

### Research Methodology

**Benchmark Dataset:**
- Create 200 text samples from academic papers
- Annotate ground truth entities (authors, papers, concepts, venues)
- Ensure diversity of entity types and difficulty levels
- Include edge cases (abbreviations, multi-word entities)

**Testing Approach:**
- Extract entities using each LLM provider
- Measure F1 score (precision and recall)
- Benchmark latency per extraction
- Calculate cost per extraction
- Analyze confidence score distributions
- Test batch processing efficiency

**Evaluation Dimensions:**
- F1 Score: Overall and per entity type
- Cost: Per extraction and per 1K tokens
- Latency: Time per extraction, batching impact
- Confidence: Calibration quality (ECE metric)
- Consistency: Same entity extracted identically
- Edge cases: Abbreviations, acronyms, domain terms

### Expected Output

â‰¥3,500 word technical report including:
- Provider benchmark matrix with F1, cost, latency
- Pareto frontier: best accuracy at each cost level
- Cost savings analysis for cheaper alternatives
- Confidence score calibration assessment
- Batch processing cost reduction analysis
- Clear recommendation with implementation guidance
