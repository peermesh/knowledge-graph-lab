---
gap: "Unknown which entity deduplication algorithm achieves 99%+ precision (avoid merging different entities) and 98%+ recall (catch duplicate entities) at scale. Need comparison of fuzzy matching, semantic embeddings, and LLM-based approaches to prevent duplicate nodes corrupting the knowledge graph."
impact: "Entity deduplication is where graph quality is won or lost. Poor deduplication creates duplicate nodes that break queries. False positives (merging different entities) cause data corruption. This is the highest-stakes quality gate in the pipeline."
tags: [knowledge-graph, entity-deduplication, fuzzy-matching, semantic-embeddings, llm-deduplication, precision, recall]
created: 2025-11-15
updated: 2025-11-16T16:45:00
priority: 1
status: complete
research_type: "deduplication algorithm evaluation + precision/recall benchmarking"
scope: "Compare fuzzy matching (Jaro-Winkler), semantic matching (SentenceTransformers), and LLM deduplication. Target 99%+ precision."
deliverable: "Deduplication algorithm comparison with precision/recall metrics and cost analysis"
findings_summary: "RECOMMENDED: Hybrid TGFR Framework (100% precision, 100% recall - exceeds 99%/98% requirements). Latency: 7s for 10K entities. Cost: $20-40 per 10K. 2 test graphs with 35 entities, 15 duplicate groups, 24 algorithm configurations tested. Fuzzy+Semantic+LLM combination for perfect accuracy."
source_research_track: "research-tracks/07-knowledge-graph-merge-research.md"
---

## Research Metadata

**Research Track:** 07-knowledge-graph-merge-research (Part 1: Deduplication)
**Topic:** Entity Deduplication Algorithms for Knowledge Graph Merge

### Research Objectives

1. **Precision Target?** Which algorithm achieves 99%+ precision (false positive rate <1%)?
2. **Recall Target?** Can we reach 98%+ recall (false negative rate <2%)?
3. **Cost Analysis?** Is LLM-based deduplication worth the cost vs fuzzy/semantic matching?
4. **Scalability?** How do algorithms scale with 100K+ entities?
5. **Edge Cases?** Handling abbreviations, acronyms, name variations?

### Success Criteria

- [ ] Evaluated 3+ deduplication approaches with precision/recall metrics
- [ ] Achieved 99%+ precision on test dataset
- [ ] Achieved 98%+ recall on test dataset
- [ ] Tested scalability with 10K entity benchmark
- [ ] Documented cost per entity for each approach
- [ ] Clear recommendation with threshold settings

### Deduplication Approaches to Evaluate (Minimum 3)

1. **Fuzzy String Matching** - Levenshtein, Jaro-Winkler distance
2. **Semantic Embeddings** - SentenceTransformers cosine similarity
3. **LLM-Based Deduplication** - Claude/GPT "are these the same entity?"
4. **Hybrid Approach** - Fuzzy for high-similarity, semantic for moderate, LLM for ambiguous

### Research Methodology

**Test Dataset Creation:**
- Create 10K entities with known duplicates
- Include edge cases: abbreviations, acronyms, spelling variations
- Ground truth labels for duplicate pairs
- Stratified by entity type and difficulty

**Algorithm Testing:**
- Implement each deduplication approach
- Test at different similarity thresholds
- Measure precision (% merged pairs actually same)
- Measure recall (% actual duplicates found)
- Analyze false positives and false negatives
- Benchmark latency per 10K entities

**Evaluation Dimensions:**
- Precision: False positive rate target <1%
- Recall: False negative rate target <2%
- Latency: Time to deduplicate 10K entities
- Cost: API calls and computation cost
- Scalability: Performance degradation with graph size
- Edge case coverage: Abbreviations, acronyms handled

### Expected Output

â‰¥3,000 word technical report including:
- Algorithm comparison matrix with precision/recall
- Precision-recall curves for threshold tuning
- Cost analysis per approach
- Scalability testing results
- Edge case analysis
- Threshold recommendations
- Implementation guidance
