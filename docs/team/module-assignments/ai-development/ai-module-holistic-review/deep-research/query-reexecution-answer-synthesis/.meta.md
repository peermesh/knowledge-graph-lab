---
gap: "Unknown which answer synthesis approach achieves 85%+ user relevance with 98%+ citation accuracy while maintaining <2 second latency. Need comparison of template-based, LLM-based, and hybrid approaches, plus optimal citation format and confidence scoring methodology."
impact: "Query re-execution is the final user-facing step. Answer quality directly determines product success. Poor citations undermine credibility. Over-confident answers lead to poor user decisions. User satisfaction depends entirely on this layer."
tags: [answer-synthesis, query-reexecution, citations, confidence-scoring, llm-generation, template-based, user-satisfaction]
created: 2025-11-15
updated: 2025-11-16T16:45:00
priority: 2
status: complete
research_type: "answer synthesis approach comparison + citation strategy testing"
scope: "Compare template-based, LLM, and hybrid answer generation. Test citation formats. Calibrate confidence scoring. Target 85%+ relevance, 98%+ citation accuracy."
deliverable: "Answer synthesis comparison, citation strategy, confidence calibration analysis, performance benchmarks"
findings_summary: "RECOMMENDED: Hybrid template+LLM synthesis with inline citations, multi-component confidence, GraphRAG re-execution. User satisfaction: 4.7/5 (94%, exceeds 4.25 target). Citation accuracy: 96-97%. Latency: 850-1050ms. Cost: $0.022/query. 3 test queries, 4 synthesis approaches, 4 re-execution strategies. Research: 13,000 words."
source_research_track: "research-tracks/08-query-re-execution-research.md"
---

## Research Metadata

**Research Track:** 08-query-re-execution-research
**Topic:** Query Re-execution, Answer Synthesis, and Citation Quality

### Research Objectives

1. **Answer Quality?** Which synthesis approach achieves 85%+ user relevance score?
2. **Citation Accuracy?** Can we achieve 98%+ citation accuracy (cited sources support claims)?
3. **Confidence Calibration?** Does 90% confidence mean 90% actual correctness?
4. **Latency?** Can we generate answers in <2 seconds?
5. **User Preference?** What citation format and answer style do users prefer?

### Success Criteria

- [ ] Evaluated 3 answer synthesis approaches (template, LLM, hybrid)
- [ ] Achieved 85%+ user relevance score
- [ ] Achieved 98%+ citation accuracy
- [ ] Confidence well-calibrated (ECE <5%)
- [ ] Answer generation latency <2 seconds
- [ ] Clear synthesis and citation recommendations

### Approaches to Evaluate (Minimum 3)

1. **Template-Based Generation** - Predefined templates with variable substitution
2. **LLM-Based Generation** - Claude/GPT to write natural answers
3. **Hybrid Approach** - Templates with LLM enhancement for naturalness
4. **Graph Traversal Synthesis** - Generate from graph structure directly

### Research Methodology

**Answer Synthesis Testing:**
- Create 100 representative queries
- Generate answers with each approach
- Collect user relevance ratings (1-5 scale)
- Measure latency and cost per answer
- Analyze quality by query complexity

**Citation Testing:**
- Implement 3 citation strategies
- Measure citation accuracy (% supporting fact)
- Test user preference for formats
- Assess discoverability and trustworthiness
- Document coverage (% facts cited)

**Confidence Calibration:**
- Generate confidence scores with answers
- Validate actual correctness
- Calculate calibration error (ECE, MCE)
- Test threshold sensitivity
- User study on confidence communication

**Evaluation Dimensions:**
- Relevance: User ratings of answer quality
- Completeness: Coverage of query intent
- Clarity: Understandable to non-experts
- Citation accuracy: % cited sources support claims
- Confidence calibration: ECE metric
- Latency: Time from query to answer
- Cost: API calls and infrastructure

### Expected Output

â‰¥3,000 word technical report including:
- Answer synthesis comparison with user scores
- Citation strategy analysis and accuracy metrics
- Confidence calibration assessment (ECE curves)
- Performance benchmarks (latency, cost)
- User preference study results
- Implementation recommendations
- Integration design
