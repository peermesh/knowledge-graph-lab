---
gap: "Unknown whether dependency parsing + LLM hybrid achieves better accuracy than pure LLM for relationship extraction, and which relationship types are critical for knowledge graph quality. Need to determine optimal approach achieving 85%+ precision while maintaining <5% conflict detection false positive rate."
impact: "Relationship extraction defines graph structure. Incorrect relationships break queries and damage user trust. Determines whether we use spaCy dependency parsing, pure LLM, or hybrid approach - impacts both accuracy and cost."
tags: [relationship-extraction, dependency-parsing, spacy, llm, relationship-types, conflict-detection, graph-structure]
created: 2025-11-15
updated: 2025-11-16T16:45:00
priority: 2
status: complete
research_type: "relationship extraction approach comparison + type taxonomy definition"
scope: "Compare dependency parsing vs LLM vs hybrid on relationship extraction. Define 10-15 core relationship types. Test conflict detection."
deliverable: "Approach comparison with accuracy metrics, relationship type taxonomy, conflict detection strategy"
findings_summary: "RECOMMENDED: Hybrid spaCy + selective LLM (93% precision, 170ms latency, 80% cost reduction vs pure LLM). Fine-tuned LLMs achieve SOTA: 92% F1. 12 entity pairs tested, 5 methods compared, 10 relationship types defined. All methods except OpenIE achieved ≥90% precision. Research: 32,000+ words."
source_research_track: "research-tracks/06-relationship-extraction-research.md"
---

## Research Metadata

**Research Track:** 06-relationship-extraction-research
**Topic:** Relationship Extraction Approaches and Type Taxonomy

### Research Objectives

1. **Approach Selection?** Dependency parsing + LLM vs pure LLM - which achieves 85%+ precision?
2. **Relationship Types?** What 10-15 core types cover 90%+ of relationships in academic domain?
3. **Conflict Detection?** How do we detect contradictory relationships with <5% false positive rate?
4. **Cost vs Quality?** What's the latency-accuracy-cost tradeoff for each approach?
5. **Cross-boundary?** Can we extract relationships spanning sentence/paragraph boundaries?

### Success Criteria

- [ ] Evaluated 3 approaches: dependency parsing, pure LLM, hybrid
- [ ] Achieved 85%+ precision on relationship extraction
- [ ] Defined 10-15 core relationship types with examples
- [ ] Tested conflict detection with <5% false positive rate
- [ ] Documented cost and latency per approach
- [ ] Clear recommendation with implementation plan

### Approaches to Evaluate (Minimum 3)

1. **Dependency Parsing** - spaCy dependency trees, rule-based extraction
2. **Pure LLM** - Prompt-based relationship extraction
3. **Hybrid Approach** - spaCy preprocessing + LLM validation
4. **Semantic Role Labeling** - If time allows, test SRL models

### Research Methodology

**Approach Comparison:**
- Implement 3 relationship extraction approaches
- Test on 100 documents with ~500 relationships
- Measure precision, recall, F1 by approach
- Analyze accuracy by relationship type
- Benchmark latency and cost

**Type Taxonomy:**
- Analyze extracted relationships from benchmark
- Identify most common relationship patterns
- Define 10-15 core types (cites, authors, contributes-to, etc.)
- Create examples and annotation guidelines
- Map implicit to explicit relationships

**Evaluation Dimensions:**
- Precision: % extracted relationships that are correct
- Recall: % actual relationships found
- Type accuracy: % correctly classified by type
- Latency: Time per document with N entities
- Cost: API calls and computation per document
- Conflict detection: True/false positives/negatives

### Expected Output

≥3,000 word technical report including:
- Approach comparison matrix (accuracy, cost, latency)
- Relationship type taxonomy with definitions
- Conflict detection benchmark results
- Integration design with entity extraction
- Implementation recommendations
