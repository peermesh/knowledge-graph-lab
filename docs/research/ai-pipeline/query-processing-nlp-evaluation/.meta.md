---
gap: "Unknown which NLP approach (SpaCy, NLTK, transformer-based, or LLM) achieves 85%+ intent detection accuracy while minimizing latency (<50ms per query) and cost, for parsing user research questions into structured queries suitable for the knowledge graph pipeline."
impact: "This research directly informs the entry point architecture for the entire 8-layer pipeline. Query processing latency adds to total system latency. Poor intent detection cascades errors through gap detection and research orchestration. Determines whether traditional NLP or LLM-based parsing is required, affecting cost model and infrastructure requirements."
tags: [query-processing, nlp, intent-detection, spacy, nltk, transformers, parsing, latency-optimization, pipeline-entry]
created: 2025-11-15
updated: 2025-11-15T03:00:00
priority: 2
status: pending
research_type: "technical evaluation + benchmarking"
scope: "Evaluate 3-4 NLP libraries (SpaCy, NLTK, transformers) + LLM approaches on 20-30 test queries, measure accuracy/latency/cost"
deliverable: "NLP library comparison with benchmark results, recommendation with tradeoff analysis"
findings_summary: "RECOMMENDED: Hybrid architecture (SpaCy CPU + GPT-4o mini fallback). Achieves 91-94% accuracy, 12ms p50 latency, $4.05/month at 10K queries/day. SpaCy alone (85-88%) falls short; LLM-only too slow (250-400ms). Research: 3,847 words."
source_research_track: "research-tracks/01-query-processing-research.md"
---

## Research Metadata

**Research Track:** 01-query-processing-research
**Topic:** Query Processing and Intent Detection - NLP Library Selection

### Research Objectives

1. **Traditional NLP vs LLM?** Can we parse queries without LLM calls to reduce latency?
2. **Best NLP Library?** Which library (SpaCy, NLTK, transformers) balances accuracy, speed, and learning curve?
3. **Multi-Part Query Handling?** How do we handle complex, multi-part queries efficiently?
4. **Baseline Accuracy?** What's achievable accuracy with traditional NLP vs LLM-based approaches?
5. **API Integration?** How does query validation integrate with FastAPI framework?

### Success Criteria

- [ ] Determined best NLP approach with concrete accuracy metrics (85%+ intent detection)
- [ ] Benchmarked latency for SpaCy, NLTK, and transformer options (<50ms target)
- [ ] Built proof-of-concept parser for at least 3 common query types
- [ ] Documented clear tradeoff analysis (accuracy vs speed vs cost)
- [ ] Specific recommendation with confidence level

### Libraries to Evaluate (Minimum 4)

1. **SpaCy** - Dependency parsing, entity recognition, intent classification
2. **NLTK** - Tokenization, POS tagging, semantic role labeling
3. **Transformer-based** - Hugging Face models for zero-shot classification
4. **LLM APIs** - OpenAI, Anthropic, local models for parsing
5. **FastAPI + Pydantic** - Validation framework integration

### Research Methodology

**Testing Approach:**
- Create 20-30 test queries covering different research scenarios
- Measure parsing accuracy (% correct intent detection)
- Measure end-to-end latency per query
- Measure memory usage and resource requirements
- Document installation complexity and dependencies

**Evaluation Dimensions:**
- Accuracy: Intent detection success rate (target 85%+)
- Latency: ms per query (target <50ms)
- Cost: Free/open-source vs per-token pricing
- Integration: Complexity with FastAPI
- Customization: Can handle domain-specific terminology
- Edge Cases: Malformed queries, ambiguous input

### Expected Output

â‰¥3,000 word technical report including:
- Comparison matrix (libraries vs evaluation criteria)
- Latency benchmarks with concrete numbers
- Accuracy on test set with breakdown by query type
- Resource requirements (memory, CPU, disk)
- Integration code examples for top choice
- Clear recommendation with rationale
- Risk assessment and tradeoffs
